# Arquitetura e Decis√µes de Design (FAQ)

Este documento detalha as decis√µes arquiteturais, garantias e limita√ß√µes do sistema `pg-transactional-outbox`.

---

## üß± BLOCO 1 ‚Äî Identidade do Sistema

**O sistema continua assumindo PostgreSQL como requisito obrigat√≥rio?**
Sim e n√£o. O driver √© abstra√≠do via `SqlExecutor`, permitindo teoricamente outros bancos SQL. Por√©m, o reposit√≥rio padr√£o (`PostgresOutboxRepository`) utiliza sintaxe espec√≠fica do Postgres (`FOR UPDATE SKIP LOCKED`, `RETURNING *`, `gen_random_uuid`), tornando o port para MySQL/Oracle n√£o-trivial sem reescrever as queries.

**Existe alguma parte do design que funciona sem SKIP LOCKED?**
N√£o o `claimBatch`. A implementa√ß√£o atual depende fortemente de `FOR UPDATE SKIP LOCKED` para garantir que m√∫ltiplos workers n√£o processem os mesmos eventos simultaneamente sem travar a tabela inteira. Backends sem essa feature exigiriam uma estrat√©gia de "tenta pegar lock" muito menos eficiente.

**Existe algum fluxo que dependa de features ausentes no PostgreSQL?**
A funcionalidade de **particionamento autom√°tico** (`pg_partman`) depende de uma extens√£o que n√£o vem habilitada por padr√£o em todos os DBs gerenciados (ex: RDS suporta, mas precisa ativar). O c√≥digo TypeScript n√£o gerencia a cria√ß√£o de parti√ß√µes, assume que o banco cuida disso.

**O modelo continua sendo Transactional Outbox e n√£o um broker?**
Sim. O foco √© a garantia de entrega da *fonte* (banco) para o *destino*. O "Worker" atua como um *Relay* (transportador), n√£o como um Broker de mensagens complexo com roteamento din√¢mico.

**H√° alguma promessa de exactly-once?**
N√£o. A garantia √© estritamente **at-least-once** (pelo menos uma vez). Falhas de rede ou crashes ap√≥s o efeito colateral mas antes do commit podem causar duplicatas. A idempot√™ncia deve ser tratada no consumidor.

---

## üîí BLOCO 2 ‚Äî Garantias de Concorr√™ncia

**Onde o lease √© adquirido?**
No m√©todo `claimBatch` do reposit√≥rio. O lease √© definido pela coluna `locked_until` e validado pelo `lock_token`.

**Como a expira√ß√£o do lease √© tratada?**
Passivamente. Se `locked_until < NOW()`, o evento torna-se eleg√≠vel para ser pego por *outro* worker (ou pelo mesmo). O `Reaper` (se ativo) tamb√©m pode resetar explicitamente esses eventos para `PENDING`.

**O que impede um worker antigo de continuar executando ap√≥s perder o lease?**
O `lock_token` (fencing token). Toda opera√ß√£o de atualiza√ß√£o (`markCompleted`, `markFailed`) exige que o `lock_token` enviado combine com o do banco. Se o lease expirou e outro worker pegou o evento, o `lock_token` no banco mudou, e a query do worker antigo afetar√° 0 linhas (falha otimista).

**Existe fencing token? Onde ele √© validado?**
Sim, a coluna `lock_token` (BigInt). Ele √© validado na cl√°usula `WHERE` de todas as opera√ß√µes de mudan√ßa de estado (`UPDATE outbox ... WHERE id = $1 AND lock_token = $2`).

**Ele protege apenas o banco ou tamb√©m efeitos externos?**
Apenas o banco. O worker pode ter executado o efeito externo (ex: chamada HTTP), mas falhar√° ao tentar commitar o sucesso no banco se perdeu o lease. Isso gera a duplicidade garantida "at-least-once".

---

## üîÅ BLOCO 3 ‚Äî Idempot√™ncia

**A idempot√™ncia √© por consumidor?**
Sim. A tabela `inbox` possui chave composta `(tracking_id, consumer_id)`.

**Onde √© armazenada?**
Na tabela `inbox` do banco de dados do consumidor.

**O resultado da execu√ß√£o √© guardado ou apenas a presen√ßa?**
Apenas a presen√ßa (`processed_at`). O payload da resposta n√£o √© armazenado. O objetivo √© evitar reprocessamento, n√£o servir como cache de resposta.

**Um novo consumidor consegue iniciar processamento hist√≥rico?**
Se ele usar um `consumer_id` novo, sim, o `IdempotencyStore` n√£o ter√° registros para ele. Por√©m, o *OutboxWorker* processa a fila sequencialmente/em lote. Se os eventos j√° foram marcados como `COMPLETED` no outbox, o novo consumidor n√£o os receber√° a menos que o *Outbox* seja reconfigurado para reenviar ou se utilize um *fan-out* para um broker real antes. **Nota:** No modelo atual, o worker consome e marca como completo. Se houver m√∫ltiplos *handlers* l√≥gicos dentro do mesmo worker, eles compartilham o sucesso do processamento do evento.

---

## ‚ò†Ô∏è BLOCO 4 ‚Äî Dead Letter

**Quem √© dono da DLE?**
O pr√≥prio banco (tabela `outbox`, status `DEAD_LETTER`). N√£o h√° fila separada.

**Qual SLA de tratamento?**
Indefinido pelo sistema. Eventos ficam l√° at√© interven√ß√£o manual ou expurgo.

**Existe redrive?**
SQL manual (`docs/outbox-schema.sql` tem uma query preparada `redrive_dle`) ou script customizado. N√£o h√° API autom√°tica de redrive no momento.

**Existe expurgo?**
Sim, `cleanup()` remove eventos `COMPLETED` e `DEAD_LETTER` antigos.

**Como auditoria futura acessa dados removidos?**
Se o expurgo rodar, os dados somem. Para auditoria de longo prazo, deve-se habilitar a tabela de auditoria (`outbox_audit_log` via triggers) ou fazer backup/CDC para Data Lake antes do expurgo.

---

## üì¶ BLOCO 5 ‚Äî Dados e Crescimento

**O sistema assume particionamento?**
O schema SQL (`outbox-schema.sql`) define particionamento nativo por Range (`created_at`).

**Quem cria parti√ß√µes futuras?**
O sistema assume o uso de `pg_partman` (definido no SQL) para criar parti√ß√µes periodicamente. A aplica√ß√£o Node.js **N√ÉO** cria parti√ß√µes.

**O que acontece se n√£o existirem?**
O insert falhar√° com erro do Postgres se n√£o houver parti√ß√£o cobrindo a data atual (a menos que exista parti√ß√£o `DEFAULT`, que n√£o √© recomendada com `pg_partman` para performance).

**Existe pol√≠tica de reten√ß√£o?**
Sim, configur√°vel no `pg_partman` (ex: "30 days"). O m√©todo `cleanup` da aplica√ß√£o serve para limpeza l√≥gica (soft retention) se o particionamento n√£o estiver em uso.

**Existe cold storage?**
N√£o nativo. Depende da estrat√©gia de backup do DBA.

---

## üìä BLOCO 6 ‚Äî Observabilidade Operacional

**Quais m√©tricas indicam satura√ß√£o do banco?**
Aumento no tempo de execu√ß√£o das queries (`avg_latency_seconds`), crescimento do backlog (`getPendingCount`), e alta contagem de *dead tuples* (monitorado via SQL de bloat).

**O que acontece quando backlog cresce?**
A lat√™ncia de entrega aumenta. O sistema continua funcionando, mas o atraso entre `created_at` e `processed_at` cresce (Lag).

**H√° automa√ß√£o ou s√≥ dashboard?**
S√≥ dashboard e queries de monitoramento. Autoscaling de workers deve ser externo (K8s HPA baseado na m√©trica de Lag).

**Existe medi√ß√£o de idade da fila?**
Sim, `getOldestPendingAgeSeconds()` retorna a idade do evento mais antigo pendente.

**Existe alerta de starvation?**
Visualmete no Dashboard (eventos travados em `PROCESSING` por muito tempo).

---

## üß† BLOCO 7 ‚Äî Ordena√ß√£o e Sem√¢ntica

**O sistema promete ordem global?**
N√ÉO, se houver m√∫ltiplos workers (concorr√™ncia > 1). Dentro de um √∫nico worker (concorr√™ncia = 1), a entrega √© "quase" ordenada, mas retries de falhas quebram a ordem estrita (eventos novos podem passar na frente de um antigo que falhou e est√° em backoff).

**Ele educa consumidores sobre reordena√ß√£o?**
A documenta√ß√£o impl√≠cita √© "Order is generally preserved but not guaranteed due to retries and parallelism".

**Existe estrat√©gia para ignorar eventos antigos?**
N√£o autom√°tica. O consumidor deve verificar `created_at` se a ordem for cr√≠tica ("Last Write Wins" l√≥gico).

---

## üí• BLOCO 8 ‚Äî Falhas Reais

**O que acontece se o worker cair ap√≥s efeito externo?**
O lease expira. Outro worker pega o evento. Executa o efeito de novo. √â o cen√°rio cl√°ssico de "At-Least-Once".

**Existe commit gap tracking?**
SQL de "Gap Detection" est√° dispon√≠vel nos docs, mas a aplica√ß√£o n√£o monitora gaps em tempo real. Gaps de ID s√£o normais em Postgres (rollbacks, concorr√™ncia).

**H√° como diferenciar ‚Äútentou‚Äù de ‚Äúconfirmado‚Äù?**
Sim, `retry_count > 0` indica que houve tentativa falha (ou crash) anterior.

---

## üß¨ BLOCO 9 ‚Äî Snapshot & Replay

**Existe versionamento de snapshot?**
N√£o.

**√â poss√≠vel rebuild global?**
Apenas se os eventos n√£o tiverem sido expurgados. Se houve expurgo (`cleanup`), o hist√≥rico foi perdido.

**Lazy rebuild pode gerar pico?**
Sim, reprocessar hist√≥rico gera carga massiva de leitura e escrita (update status).

**Existe controle disso?**
N√£o embutido. Scripts de replay devem ser rodados com cuidado.

---

## üîÑ BLOCO 10 ‚Äî Migra√ß√£o futura

**Como CDC ser√° introduzido?**
A estrutura atual facilita **Debezium**: ele pode ler a tabela `outbox` diretamente (connector outbox). A aplica√ß√£o apenas insere na `outbox` e o Debezium transmite, dispensando o `polling/notify relay`.

**H√° dual run?**
N√£o implementado.

**Como validar consist√™ncia?**
Audit log (`outbox_audit_log`) vs Backups.

**Quem autoriza desligar o consumo pelo banco?**
Decis√£o operacional. Basta parar os containers dos workers (`replicas: 0`) ou desabilitar o `crontab` dos scripts.

---

## üßØ BLOCO 11 ‚Äî Opera√ß√£o & Incidente

**Existem runbooks?**
As queries em `docs/outbox-schema.sql` (se√ß√£o Observabilidade) servem como base para runbooks de incidente (bloat, lag, dead letters).

**Quando pode matar backend?**
A qualquer momento. Transa√ß√µes em voo sofrem rollback. Leases expiram. O sistema se recupera sozinho.

**Como detectar vacuum starvation?**
Query de `Autovacuum lag monitoring` inclu√≠da nos docs.

**Como detectar locks longos?**
Monitoramento padr√£o do Postgres (`pg_stat_activity` com `state = 'active'` e `wait_event_type = 'Lock'`).

---

## ‚öôÔ∏è BLOCO 12 ‚Äî ORM vs SQL

**O ORM substitui queries cr√≠ticas?**
N√£o. As queries cr√≠ticas (`claimBatch` com `SKIP LOCKED`) s√£o SQL puro executado via `executor.query()`, garantindo performance e corretude que a maioria dos ORMs n√£o abstrai bem.

**O framework permite cair para SQL nativo?**
Sim, o `SqlExecutor` √© a porta de escape para SQL nativo em qualquer driver.

**√â poss√≠vel auditar exatamente o que est√° sendo executado?**
Sim, inspecionando as strings SQL nos arquivos de reposit√≥rio ou habilitando log de queries no driver/banco.
